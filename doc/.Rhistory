library(RTextTools)
library(cluster)
library(tm)
library(lmtest)
library(dendextend)
library(ggdendro)
library(factoextra)
library(fpc)
library(wordcloud)
library(stringi)
library(flashClust)
library(dynamicTreeCut)
library(textdata)
library(gridExtra)
library(grid)
full_df <- read.csv('C:/Users/dtdlu/Downloads/MICRC Map Comments.csv')
# Load dataframe
munic_full <- read.csv("C:/Users/dtdlu/Downloads/DECENNIALPL2020.P1-2023-05-01T221221.csv") %>% t() %>% as.data.frame()
munic_names_full <- row.names(munic_full)[-c(1,2)]
# Remove extraneous words and punctuation from municipality names
munic_names_full <- gsub("^St\\.\\.", "St\\.", munic_names_full)
munic_names_full <- gsub("\\.Ste\\.\\.", "Ste\\.", munic_names_full)
munic_names <- str_extract(munic_names_full, "^.+?(?=\\.\\.)")
munic_names <- gsub("\\.township|\\.city|\\.charter", "", munic_names)
munic_names <- gsub("Village|village|Village\\.of\\.", "", munic_names)
munic_names <- gsub("\\.", " ", munic_names) %>% unique()
munic_names <- munic_names[-1]
# Make all letters lowercase
munic_names <- str_to_lower(munic_names)
# Order names by length so longer names are matched first
munic_names <- munic_names[order(-nchar(munic_names))]
?nbclust
?NbClust
??NbClust
# Load dataframe
munic_full <- read.csv("C:/Users/dtdlu/Downloads/DECENNIALPL2020.P1-2023-05-01T221221.csv") %>% t() %>% as.data.frame()
munic_names_full <- row.names(munic_full)[-c(1,2)]
# Remove extraneous words and punctuation from municipality names
munic_names_full <- gsub("^St\\.\\.", "St\\.", munic_names_full)
munic_names_full <- gsub("\\.Ste\\.\\.", "Ste\\.", munic_names_full)
munic_names <- str_extract(munic_names_full, "^.+?(?=\\.\\.)")
munic_names <- gsub("\\.township|\\.city|\\.charter", "", munic_names)
munic_names <- gsub("Village|village|Village\\.of\\.", "", munic_names)
munic_names <- gsub("\\.", " ", munic_names) %>% unique()
munic_names <- munic_names[-1]
# Make all letters lowercase
munic_names <- str_to_lower(munic_names)
# Order names by length so longer names are matched first
munic_names <- munic_names[order(-nchar(munic_names))]
# Extract distinct comments and coerce to lowercase
df <- distinct(full_df, comment)
df <- mutate_all(df, tolower)
# Add an index variable
df$index <- as.character(c(1:nrow(df)))
# Restructure multi-word municipalities to create single tokens
pattern <- paste(munic_names, collapse = "|")
df$comment <- str_replace_all(df$comment, pattern, function(match) {
gsub(" ", "_", match, fixed = TRUE)})
# Identify and create data set of common stop words specific to dataset
my_stop_words <- data.frame(word = c("county", "counties", "map", "maps",
"district", "districts", "community",
"communities"),
lexicon = rep("dtl", 8))
# Filter for comments that mention municipalities
# (more likely to be feasible comments)
df_w_munic <- df[grepl(paste(pattern), df$comment),]
# Create a sample data frame
sample_df <- df_w_munic[sample(nrow(df_w_munic), 1000),] %>% as.data.frame()
# Create DTM objects for full and sample data frames
dtm <- df_w_munic %>%
unnest_tokens(word, comment, drop = F) %>%
anti_join(rbind(stop_words, my_stop_words)) %>%
count(index, word) %>%
cast_dtm(index, word, n)
sample_dtm <- sample_df %>%
unnest_tokens(word, comment, drop = T) %>%
anti_join(rbind(stop_words, my_stop_words)) %>%
count(index, word) %>%
cast_dtm(index, word, n)
dist_mat <- readRDS("redistricting_comments_distance_matrix.rds")
hc <- readRDS("redistricting_comments_hclust.rds")
plot(hc, label = F)
?fviz_nbclusy
?fviz_nbclust
cut <- cutree(hc, k = 3)
wss <- apply(as.matrix(dtm), 1,
function(x) sum((x - centers[cut[as.numeric(rownames(x))]])^2))
cut <- cutree(hc, k = 3)
centers <- aggregate(df_w_munic, list(cut), mean)[, -1]
centers
wss <- 0
clusters <- cutree(hc, k = 3)
wss <- 0
for (i in 1:3) {
cluster_indices <- which(clusters == i)
cluster_points <- dtm[cluster_indices, ]
cluster_center <- colMeans(cluster_points)
cluster_dist <- dist(cluster_points, cluster_center)
wss <- wss + sum(cluster_dist^2)
}
cluster_points
for (i in 1:3) {
cluster_indices <- which(clusters == i)
cluster_points <- dtm[cluster_indices, ] %>% as.matrix()
cluster_center <- colMeans(cluster_points)
cluster_dist <- dist(cluster_points, cluster_center)
wss <- wss + sum(cluster_dist^2)
}
cluster_points
cluster_center
dim(cluster_points)
dim(cluster_center)
for (i in 1:3) {
cluster_indices <- which(clusters == i)
cluster_points <- dtm[cluster_indices, ] %>% as.matrix()
cluster_center <- colMeans(cluster_points)
cluster_dist <- dist(cluster_points, cluster_center)
wss <- wss + sum(cluster_dist^2)
}
plot(silhouette(cutree(hc,3), dist_mat))
silhouette(cutree(hc,3), dist_mat)
silhouette(cutree(hc,3), dist_mat)[,3]
silhouette(cutree(hc,3), dist_mat)[,3] %>% mean()
for(i in 1:100){
ss <- silhouette(cutree(hc,3), dist_mat)[,3] %>% mean()
hc_ss[i] <- ss
}
hc_ss <- c()
for(i in 1:100){
ss <- silhouette(cutree(hc,3), dist_mat)[,3] %>% mean()
hc_ss[i] <- ss
}
hc_ss <- c()
for(i in 1:10){
ss <- silhouette(cutree(hc, i), dist_mat)[,3] %>% mean()
hc_ss[i] <- ss
}
for(i in 2:10){
ss <- silhouette(cutree(hc, i), dist_mat)[,3] %>% mean()
hc_ss[i] <- ss
}
hc_ss
?silhouette
for(i in 2:100){
ss <- silhouette(cutree(hc, i), dist_mat)[,3] %>% mean()
hc_ss[i] <- ss
}
plot(2:100, hc_ss)
plot(1:100, hc_ss)
?plot
plot(1:100, hc_ss,
main = "Hierarchical Silhouette Scores by Cluster Size",
xlab = "Cluster Size",
ylab = "Silhouette Score")
hc_model <- cutree(whc, 20)
hc_model <- cutree(hc, 20)
hc_model <- cutree(hc, 20)
ggplot(mapping = aes(x = as.factor(c(1:21)),
y = unname(c(table(hc_model))),
fill = rep_len(c("red", "green", "blue"),
21))) +
geom_bar(stat = "identity") +
labs(title = "Hierachical Model Cluster Sizes",
x = "Cluster",
y = "Number of Comments") +
geom_text(aes(label = unname(table(hc_model)), vjust = -.2)) +
theme(legend.position = "none")
hc_model <- cutree(hc, 20)
ggplot(mapping = aes(x = as.factor(c(1:20)),
y = unname(c(table(hc_model))),
fill = rep_len(c("red", "green", "blue"),
21))) +
geom_bar(stat = "identity") +
labs(title = "Hierachical Model Cluster Sizes",
x = "Cluster",
y = "Number of Comments") +
geom_text(aes(label = unname(table(hc_model)), vjust = -.2)) +
theme(legend.position = "none")
hc_model <- cutree(hc, 20)
ggplot(mapping = aes(x = as.factor(c(1:20)),
y = unname(c(table(hc_model))),
fill = rep_len(c("red", "green", "blue"),
20))) +
geom_bar(stat = "identity") +
labs(title = "Hierachical Model Cluster Sizes",
x = "Cluster",
y = "Number of Comments") +
geom_text(aes(label = unname(table(hc_model)), vjust = -.2)) +
theme(legend.position = "none")
length(df_w_munic)
dim(df_w_munic)
dim(full_df)
kmeans_model <- readRDS("kmeans_model.rds")
ggplot(mapping = aes(x = as.factor(c(1:20)),
y = kmeans_model$size,
fill = rep_len(c("red", "green", "blue"),
20))) +
geom_bar(stat = "identity") +
labs(title = "K-Means Model Cluster Sizes",
x = "Cluster",
y = "Number of Comments") +
geom_text(aes(label = kmeans_model$size, vjust = -.2)) +
theme(legend.position = "none")
kmeans_model <- readRDS("kmeans_model.rds")
ggplot(mapping = aes(x = as.factor(c(1:20)),
y = kmeans_model$size,
fill = rep_len(c("red", "green", "blue"),
20))) +
geom_bar(stat = "identity") +
labs(title = "K-Means Model Cluster Sizes",
x = "Cluster",
y = "Number of Comments") +
geom_text(aes(label = kmeans_model$size, vjust = -.2)) +
theme(legend.position = "none")
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
# Load dataframe
munic_full <- read.csv("C:/Users/dtdlu/Downloads/DECENNIALPL2020.P1-2023-05-01T221221.csv") %>% t() %>% as.data.frame()
munic_names_full <- row.names(munic_full)[-c(1,2)]
# Remove extraneous words and punctuation from municipality names
munic_names_full <- gsub("^St\\.\\.", "St\\.", munic_names_full)
munic_names_full <- gsub("\\.Ste\\.\\.", "Ste\\.", munic_names_full)
munic_names <- str_extract(munic_names_full, "^.+?(?=\\.\\.)")
munic_names <- gsub("\\.township|\\.city|\\.charter", "", munic_names)
munic_names <- gsub("Village|village|Village\\.of\\.", "", munic_names)
munic_names <- gsub("\\.", " ", munic_names) %>% unique()
munic_names <- munic_names[-1]
# Make all letters lowercase
munic_names <- str_to_lower(munic_names)
# Order names by length so longer names are matched first
munic_names <- munic_names[order(-nchar(munic_names))]
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
library(tidyverse)
munic_full <- read.csv("/Users/dtdlu/Downloads/DECENNIALPL2020.P1-2023-05-01T220258.csv") %>% t() %>% as.data.frame()
munic_names_full <- row.names(munic_full)[-c(1,2)]
# Remove extraneous words and punctuation from municipality names
munic_names_full <- gsub("^St\\.\\.", "St\\.", munic_names_full)
munic_names_full <- gsub("\\.Ste\\.\\.", "Ste\\.", munic_names_full)
munic_names <- str_extract(munic_names_full, "^.+?(?=\\.\\.)")
munic_names <- gsub("\\.township|\\.city|\\.charter", "", munic_names)
munic_names <- gsub("Village|village|Village\\.of\\.", "", munic_names)
munic_names <- gsub("\\.", " ", munic_names) %>% unique()
munic_names <- munic_names[-1]
munic_df <- munic_full[-c(1,2), 1:2]
munic_df$V1 <- gsub(",", "", munic_df$V1) %>% as.numeric()
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
library(tidyverse)
munic_full <- read.csv("/Users/dtdlu/Downloads/DECENNIALPL2020.P1-2023-05-01T221221.csv.csv") %>% t() %>% as.data.frame()
munic_full <- read.csv("/Users/dtdlu/Downloads/DECENNIALPL2020.P1-2023-05-01T221221.csv") %>% t() %>% as.data.frame()
library(tidyverse)
munic_full <- read.csv("/Users/dtdlu/Downloads/DECENNIALPL2020.P1-2023-05-01T221221.csv") %>% t() %>% as.data.frame()
munic_names_full <- row.names(munic_full)[-c(1,2)]
# Remove extraneous words and punctuation from municipality names
munic_names_full <- gsub("^St\\.\\.", "St\\.", munic_names_full)
munic_names_full <- gsub("\\.Ste\\.\\.", "Ste\\.", munic_names_full)
munic_names <- str_extract(munic_names_full, "^.+?(?=\\.\\.)")
munic_names <- gsub("\\.township|\\.city|\\.charter", "", munic_names)
munic_names <- gsub("Village|village|Village\\.of\\.", "", munic_names)
munic_names <- gsub("\\.", " ", munic_names) %>% unique()
munic_names <- munic_names[-1]
munic_df <- munic_full[-c(1,2), 1:2]
munic_df$V1 <- gsub(",", "", munic_df$V1) %>% as.numeric()
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
sample(row.names(munic_df), 13, prob = munic_df$V1/sum(munic_df$V1))
# Loading Data and packages
library(tidyverse)
library(ggplot2)
library(maps)
library(countrycode)
library(tm)
library(topicmodels)
library(flashClust)
library(tidytext)
library(shiny)
library(wordcloud2)
library(DT)
hdb <- read.csv("processed_moments_w_demog.csv")
renderImage(list(src="../figs/Frequency_Map.png"))
source("../doc/Top Words Shiny App.R")
shinyApp(ui, server)
renderImage(list(src="../figs/Top Terms by Topic.png"))
renderImage(list(src="../figs/Topic 1 Terms Map.png"))
renderImage(list(src="../figs/Topic 2 Terms Map.png"))
renderImage(list(src="../figs/Topic 3 Terms Map.png"))
renderImage(list(src="../figs/Topic 4 Terms Map.png"))
renderImage(list(src="../figs/Topic 5 Terms Map.png"))
source("../doc/Final Data Table Shiny.R")
library(DT)
library(shiny)
count_gamma_df <- read.csv("../output/count_gamma_df.csv")
ui <- fluidPage(
titlePanel("Searchable Data Table"),
mainPanel(
DTOutput("data_table")
)
)
server <- function(input, output) {
output$data_table <- renderDT({
datatable(
count_gamma_df,
options = list(
searching = TRUE,  # Enable search functionality
pageLength = 10    # Number of rows per page
)
)
})
}
shinyApp(ui, server)
# Loading Data and packages
library(tidyverse)
library(ggplot2)
library(maps)
library(countrycode)
library(tm)
library(topicmodels)
library(flashClust)
library(tidytext)
library(shiny)
library(wordcloud2)
library(DT)
hdb <- read.csv("processed_moments_w_demog.csv")
renderImage(list(src="../figs/Frequency_Map.png"))
source("../doc/Top Words Shiny App.R")
shinyApp(ui, server)
source("../doc/Final Data Table Shiny.R")
source("../doc/Final Data Table Shiny.R")
source("../doc/`Final Data Table Shiny.R`")
source("../doc/Final Data Table Shiny.R")
count_gamma_df <- read.csv("../output/count_gamma_df.csv")
count_gamma_df
source("../doc/Final Data Table Shiny.R")
# Loading Data and packages
library(tidyverse)
library(ggplot2)
library(maps)
library(countrycode)
library(tm)
library(topicmodels)
library(flashClust)
library(tidytext)
library(shiny)
library(wordcloud2)
library(DT)
hdb <- read.csv("processed_moments_w_demog.csv")
ggsave(filename = "Frequency_Map.png", plot = map1, width = 8, height = 4,
path = "../figs/")
# Loading Data and packages
library(tidyverse)
library(ggplot2)
library(maps)
library(countrycode)
library(tm)
library(topicmodels)
library(flashClust)
library(tidytext)
library(shiny)
library(wordcloud2)
library(DT)
hdb <- read.csv("processed_moments_w_demog.csv")
country_count <- table(hdb$country) %>% as.data.frame()
colnames(country_count) <- c("country_code", "freq")
world_map <- map_data("world")
world_map$region <- countrycode(world_map$region,
"country.name",
"iso3c")
mapdata <- left_join(world_map, country_count,
by = c("region" = "country_code"))
map1 <- ggplot(mapdata, aes(x = long, y = lat, group = group)) +
geom_polygon(aes(fill = log(freq)), color = "black") +
scale_fill_gradient(name = "Moments (Log Scale)",
low = "blue", high = "red",
na.value = "grey",
breaks = c(log(10000), log(1000), log(100), log(10)),
labels = c("10000", "1000", "100", "10"))
ggsave(filename = "Frequency_Map.png", plot = map1, width = 8, height = 4,
path = "../figs/")
source("../doc/Final Data Table Shiny.R")
setwd("GitHub/ads-fall2023-project1-dtl2129/output/")
source("../doc/Final Data Table Shiny.R")
runApp('~/GitHub/ads-fall2023-project1-dtl2129/doc/Final Data Table Shiny.R')
server <- function(input, output) {
output$data_table <- renderDT({
datatable(
count_gamma_df,
options = list(
searching = TRUE,  # Enable search functionality
pageLength = 10    # Number of rows per page
)
)
})
}
source("../doc/Final Data Table Shiny.R")
precomputed_results <- readRDS("../output/precomputed_results.rds")
precomputed_results <- readRDS("../output/precomputed_results.rds")
?write.csv
write.csv(count_gamma_df, file = "../doc/count_gamma_df.csv", )
# Loading Data and packages
library(tidyverse)
library(ggplot2)
library(maps)
library(countrycode)
library(tm)
library(topicmodels)
library(flashClust)
library(tidytext)
library(shiny)
library(wordcloud2)
library(DT)
hdb <- read.csv("processed_moments_w_demog.csv")
# Loading Data and packages
library(tidyverse)
library(ggplot2)
library(maps)
library(countrycode)
library(tm)
library(topicmodels)
library(flashClust)
library(tidytext)
library(shiny)
library(wordcloud2)
library(DT)
hdb <- read.csv("../doc/processed_moments_w_demog.csv")
country_count <- table(hdb$country) %>% as.data.frame()
colnames(country_count) <- c("country_code", "freq")
world_map <- map_data("world")
world_map$region <- countrycode(world_map$region,
"country.name",
"iso3c")
mapdata <- left_join(world_map, country_count,
by = c("region" = "country_code"))
map1 <- ggplot(mapdata, aes(x = long, y = lat, group = group)) +
geom_polygon(aes(fill = log(freq)), color = "black") +
scale_fill_gradient(name = "Moments (Log Scale)",
low = "blue", high = "red",
na.value = "grey",
breaks = c(log(10000), log(1000), log(100), log(10)),
labels = c("10000", "1000", "100", "10"))
ggsave(filename = "Frequency_Map.png", plot = map1, width = 8, height = 4,
path = "../figs/")
lda_model <- readRDS("../doc/lda_model.rds")
# Apply LDA
num_topics <- 5
lda_model <- LDA(dtm, k = num_topics, control = list(seed=356246),
method = "Gibbs")
# Full DTM
corpus <- Corpus(VectorSource(hdb$text))
dtm <- DocumentTermMatrix(corpus,
control = list(wordLengths = c(1, Inf)))
tfidf <- weightTfIdf(dtm)
# Apply LDA
num_topics <- 5
lda_model <- LDA(dtm, k = num_topics, control = list(seed=356246),
method = "Gibbs")
saveRDS(lda_model, "../doc/lda_model.rds")
tidy_lda <- tidy(lda_model)
top_terms <- tidy_lda %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
ggsave("Top Terms by Topic.png", path = "../figs/")
country_comment_counts <- hdb %>%
group_by(country) %>%
summarise(`Comment Count` = n())
count_gamma_df <- left_join(country_avg_gammas, country_comment_counts)
lda_model <- readRDS("../doc/")
lda_model <- readRDS("../doc/lda_model.rds")
gammas <- tidy(lda_model, matrix = "gamma")
gammas_wider <- pivot_wider(gammas, id_cols = document,
names_from = topic,
values_from = gamma)
gammas_country <- cbind(gammas_wider, hdb$country)
topic_cats <- c("Shopping", "At Home with Family",
"Time with Friends", "Food", "Generic Happiness")
colnames(gammas_country) <- c("document",
paste0("Topic ", c(1:5),
" ", "(", c(topic_cats), ")"),
"country")
country_avg_gammas <- gammas_country %>%
group_by(country) %>%
summarize(
`Topic 1 (Shopping)` = mean(`Topic 1 (Shopping)`, na.rm = TRUE),
`Topic 2 (At Home with Family)` = mean(`Topic 2 (At Home with Family)`,
na.rm = TRUE),
`Topic 3 (Time with Friends)` = mean(`Topic 3 (Time with Friends)`,
na.rm = TRUE),
`Topic 4 (Food)` = mean(`Topic 4 (Food)`, na.rm = TRUE),
`Topic 5 (Generic Happiness)` = mean(`Topic 5 (Generic Happiness)`,
na.rm = TRUE))
country_comment_counts <- hdb %>%
group_by(country) %>%
summarise(`Comment Count` = n())
count_gamma_df <- left_join(country_avg_gammas, country_comment_counts)
count_gamma_df$country.name <- countrycode(count_gamma_df$country,
"iso3c", "country.name")
write.csv(count_gamma_df, file = "../doc/count_gamma_df.csv", )
saveRDS(dtm, "../doc/dtm.rds")
saveRDS(dtm,"../doc/tfidf.rds")
